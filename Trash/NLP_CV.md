# Paper List
- [NLP](#nlp)
  - [Pretraining](#pretraining)
- [Weak Supervision](#weak-supervision)
- [Few-Shot Learning](#few-shot-learning)
- [Transfer Learning](#transfer-learning)
- [Meta Learning](#meta-learning)
- [Machine Learning](#machine-learning)
  - [Data Manipulation](#data-manipulation)
  - [Noisy Supervision](#noisy-supervision)
- [Reinforcement Learning](#reinforcement-learning)
- [CV](#cv)
- [Other Resource](#other-resource)

## NLP


### Pretraining
- Transformer: Attention Is All You Need. *NeuIPS 2017.* [[Paper]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
- SciBERT: A Pretrained Language Model for Scientific Text. *ACL 2019.* [[Paper]](https://arxiv.org/pdf/1903.10676.pdf)
- GPT: Improving Language Understanding by Generative Pre-Training. [[Paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- GPT-2: Language Models are Unsupervised Multitask Learners. [[Paper]](http://www.persagen.com/files/misc/radford2019language.pdf)
- GPT-3: Language Models are Few-Shot Learners. [[Paper]](https://arxiv.org/pdf/2005.14165.pdf)
- OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space. *Li, Chunyuan and Gao, Xiang and Li, Yuan and Li, Xiujun and Peng, Baolin and Zhang, Yizhe and Gao, Jianfeng.* arXiv 2020. [[Paper]](https://arxiv.org/pdf/2004.04092.pdf)
- Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. *Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung.* arXiv 2020 [[paper]](https://arxiv.org/pdf/2007.15779.pdf)


## Weak Supervision

- Learning from Noisy Labels with Deep Neural Networks: A Survey. *arXiv 2020.* [[Paper]](https://arxiv.org/pdf/2007.08199.pdf) ðŸ¤”


## Few-Shot Learning

- Learning from Very Few Samples: A Survey. *arXiv 2020.*. [[Paper]](https://arxiv.org/pdf/2009.02653.pdf) [[Note]](./PaperNote/2020_0911_0917.md)




## Transfer Learning

- Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks. *ACL 2020.* [[Paper]](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)[[Note]](./PaperNote/2020_0918_0924.md)

## Meta Learning

- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. *ICML 2017.* [[Paper]](https://arxiv.org/pdf/1703.03400.pdf) [[Note]](./PaperNote/2020_0903_0910.md) [[Video]](https://www.bilibili.com/video/av46561029?p=40)
- On First-Order Meta-Learning Algorithms. *arXiv 2018.* [[Paper]](https://arxiv.org/pdf/1803.02999.pdf)  [[Note]](./PaperNote/2020_0903_0910.md) [[Video]](https://www.bilibili.com/video/av46561029?p=40)
- Yet Meta Learning Can Adapt Fast, It Can Also Break Easily. *arXiv 2020.* [[Paper]](https://arxiv.org/pdf/2009.01672.pdf) [[Note]](./PaperNote/2020_0903_0910.md)
- Learning to Reweight Examples for Robust Deep Learning. *ICML 2018.* [[Paper]](https://arxiv.org/pdf/1803.09050.pdf) [[Note]](./PaperNote/2020_0903_0910.md) [[Video]](https://vimeo.com/287808016)
- Optimizing Data Usage via Differentiable Rewards. *ICML 2019.* [[Paper]](https://arxiv.org/pdf/1911.10088.pdf)
- Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting. *NeurIPS 2019.* [[Paper]](https://arxiv.org/pdf/1902.07379.pdf) [[Note]](./PaperNote/2020_0903_0910.md)

## Machine Learning

- Understanding black-box predictions via influence functions. *ICML 2017.* [[[Paper]]](https://arxiv.org/pdf/1703.04730.pdf) [[Note]](./PaperNote/2020_0831_0906.md)  [[Video]](https://www.youtube.com/watch?v=0w9fLX_T6tY) [[Slide]](https://drive.google.com/file/d/1ZLY_9Wsk9MA0kXAoJDd6o1gbLvHhyPAn/view)


### Data Manipulation

- Less Is Better: Unweighted Data Subsampling via Influence Function. *AAAI 2020.* [[Paper]](https://arxiv.org/pdf/1912.01321.pdf) [[Note]](./PaperNote/2020_0903_0910.md)
- Enhancing Simple Models by Exploiting What They Already Know. *ICML 2020.* [[Paper]](https://proceedings.icml.cc/static/paper_files/icml/2020/126-Paper.pdf) ðŸ¤”
- Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting. *NeurIPS 2019.* [[Paper]](https://arxiv.org/pdf/1902.07379.pdf) [[Note]](./PaperNote/2020_0903_0910.md)
- Learning Data Manipulation for Augmentation and Weighting. *NeurIPS 2019.* [[Paper]](https://arxiv.org/pdf/1910.12795.pdf) [[Note]](./PaperNote/2020_0903_0910.md)
- **Neural Data Augmentation via Example Extrapolation.** *Lee, Kenton and Guu, Kelvin and He, Luheng and Dozat, Tim and Chung, Hyung Won.* arXiv 2021. [[paper]](https://arxiv.org/pdf/2102.01335.pdf)



### Noisy Supervision

- A Simple yet Effective Baseline for Robust Deep Learning with Noisy Labels. *arXiv 2019.* [[Paper]](https://arxiv.org/pdf/1909.09338.pdf) ðŸ¤”
- Masking: A New Perspective of Noisy Supervision. *NeurIPS 2019.* [[Paper]](https://arxiv.org/pdf/1805.08193.pdf) ðŸ¤”
- Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels. *NeurIPS 2018.* [[Paper]](https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf) [[Note]](./PaperNote/2020_0903_0910.md)
- Searching to Exploit Memorization Effect in Learning with Noisy Labels. *ICML 2020.* [[Paper]](https://arxiv.org/pdf/1911.02377.pdf) ðŸ¤”
- MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels. *ICML 2018.* [[Paper]](https://arxiv.org/pdf/1712.05055.pdf) ðŸ¤”

### Contrastive Learning

**Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning.** *Gunel, Beliz and Du, Jingfei and Conneau, Alexis and Stoyanov, Ves.* arXiv 2020. [[paper]](https://arxiv.org/pdf/2011.01403.pdf) [[note]](./PaperNote/2020_1112_1119.md)


## Reinforcement Learning

- Connecting the Dots Between MLE and RL for Sequence Prediction. *arXiv 2019.* [[Paper]](https://arxiv.org/pdf/1811.09740.pdf) [[Note]](./PaperNote/2020_0903_0910.md)

## CV

- A Framework For Contrastive Self-Supervised Learning And Designing A New Approach. *ArXiv 2020.* [[Paper]](https://arxiv.org/pdf/2009.00104.pdf)
- Momentum Contrast for Unsupervised Visual Representation Learning. *ArXiv 2020.* [[Paper]](https://arxiv.org/pdf/1911.05722.pdf) [[Note]](./PaperNote/2020_0925_0930.md)

## Other Resource

- [Author List](./Author_List.md)
- [English Collection](./English_Collection.md)
- Paper Digest: ICML 2020 Highlights. [[Link]](https://www.paperdigest.org/2020/07/icml-2020-highlights/)
