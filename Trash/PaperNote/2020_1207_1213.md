# Paper Weekly

> 2020/12/07-12/13

|Index|Area|Title & Source|Idea|
|:---:|:--|:-------------|:---|
|1|Weak Supervision Neu-IR|**Learning to Learn from Weak Supervision by Full Supervision.** NIPS 2017 Workshop. *Dehghani, Mostafa and Severyn, Aliaksei and Rothe, Sascha and Kamps, Jaap.* [[paper]](https://arxiv.org/pdf/1711.11383.pdf)|利用multi-task training同时训练两个网络，一个trained on weak data, 另一个trained on labeled data. 跟ren's reweight是不一样的，两个network有各自的loss, 并且weak data也是有真实label的. 做的情感分类。但是他这个想法其实与我们的比较接近, 里面一些阐述都指得借鉴|
||
|2|Weak Supervision Neu-IR|**Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2.** *Mass, Yosi and Roitman, Haggai.* EMNLP 2020. [[paper]](https://www.aclweb.org/anthology/2020.emnlp-main.343.pdf)| 与Ma的有3点不同：(1) Ma used FAQ dataset, its answer text is short (2) train a model to generate title paraphrases that can be used for query-abstract matching and query-to-title matching (3) taking only pairs that were voted by at least one-user on those Community QA (CQA) sites. **Note** its expremnetal setting 可以借鉴 about COVID dataset, BERT and GPT2, 我们相当于使用了web, news, covid|
||
|3|Dense Retrieval|**Pre-training Tasks for Embedding-based Large-scale Retrieval.** *Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar.* [[paper]](https://arxiv.org/pdf/2002.03932.pdf) ICLR 2020. |While their method can be used as an initial retrieval, the authors still require an additional re-ranking step.|
