# Paper Weekly

> 2020/9/3-9/10

|Index|Area|Source|Title|Idea|
|:---:|:---------:|:--|:---|:----|
|1|Data Subsampling|AAAI 2020|Less Is Better: Unweighted Data Subsampling via Influence Function [~Paper](https://arxiv.org/pdf/1912.01321.pdf)|**What:** 在Big Data上训练complex model是很困难的，本文提出了一种data subsampling的方法旨在利用subset data达到超越full set的效果. **How:** 利用Influence function来获得each training point对entire test loss的影响 (weight)，然后根据此影响对training set进行采样|
|2|Data Subsampling|NeurIPS 2019|Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting [~Paper](https://arxiv.org/pdf/1902.07379.pdf)|针对DNN容易overfit corrupted labels & class imbalance in training data，提出了一种adaptively reweight的方法，根据training loss and dev loss来优化生成weight的small Meta weight net (MLP)|
|3|Meta Learning|ICML 2017|Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [~Paper](https://arxiv.org/pdf/1703.03400.pdf)|假设一个batch含有m个data point, 传统machine learning直接取这m个 loss的平均值进行一次update，但是MAML是分别对这m个loss更新得到m个updated models, 然后再在一个meta batch(可以理解为mini-dev)上求一个meta loss，得到用于真正update的parameter gradients (second order). 作者在5.2节证明了对second order gradient进行一阶近似效果几乎差不多，但是计算开销大大减少。|
|4|Data Manipulation|NeurIPS 2019|Learning Data Manipulation for Augmentation and Weighting [~Paper](https://arxiv.org/pdf/1910.12795.pdf)|前人的data manipulation Algorithms往往只能完成data augmentation 或 data weighting, this paper可以利用一种算法同时实现两种不同的data manipulation (data augmentation: 在不改变label的情况下，perturbs examples以此获得更多的training samples). 个人认为比较有意思的一点:他们认为Maximum Likelihood Learning可以被看做是reinforcement learning的一种特例，因此本文将MLL as a reward learning. (这块还没有完全看懂，需要再好好研究下)|
|5|Meta-Learning|arXiv 2020|Yet Meta Learning Can Adapt Fast, It Can Also Break Easily [~Paper](https://arxiv.org/pdf/2009.01672.pdf)|
|6|Meta-Learning|arXiv 2018|On First-Order Meta-Learning Algorithms.  [~Paper](https://arxiv.org/pdf/1803.02999.pdf)|
|7|Data Reweight|ICML 2018|Learning to Reweight Examples for Robust Deep Learning. [~Paper](https://arxiv.org/pdf/1803.09050.pdf)|又重读了一遍，它在ICML2018上的talk把我讲明白了[~Video](https://vimeo.com/287808016)|
|8|Noisy Supervision|ICML 2020|Searching to Exploit Memorization Effect in Learning with Noisy Labels. [~Paper](https://arxiv.org/pdf/1911.02377.pdf)|与10th Co-training的工作一样，都是利用memorization effect进行much noisy data selection. 与Co-training的maually设定drop schedule不同，this paper利用AutoML(类似于neural architure search, NAS)来自动search schedule based on learning curves. 关注到的一点, 本文对"to reduce the negative effects of noisy label"的工作进行了分类: (1) label transition matrix (2) regularization (3) selection / weighting|
|9|Sequence Prediction|arXiv 2019|Connecting the Dots Between MLE and RL for Sequence Prediction [~Paper](https://arxiv.org/pdf/1811.09740.pdf)|
|10|Noisy Supervision|NeurIPS 2018|Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels [~Paper](https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf)|针对极度noisy的情况(45%噪声)，提出了一种co-training的方法: 同时训练两个一样的models A & B，每个batch中，A使用B中loss小instance训练，B利用A中loss小的进行update. 本文设置了一个data drop schedule: 开始训练时model不会记住noisy data,因此选择保留大部分数据，而as training epoch proceeds，model会对噪声逐渐进行拟合，所以drop more data. (这个schedule是根据memorization effect设计的: deep model can memorize easy instances first, and gradually adapt to hard instances during the training)|
