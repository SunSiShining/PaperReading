# Paper Weekly

> 2020/9/3-9/10

|Index|Area|Source|Title|Idea|
|:---:|:---------:|:--|:---|:----|
|1|Data Subsampling|AAAI 2020|Less Is Better: Unweighted Data Subsampling via Influence Function [~Paper](https://arxiv.org/pdf/1912.01321.pdf)|**What:** 在Big Data上训练complex model是很困难的，本文提出了一种data subsampling的方法旨在利用subset data达到超越full set的效果. **How:** 利用Influence function来获得each training point对entire test loss的影响 (weight)，然后根据此影响对training set进行采样|
|2|Data Subsampling|NeurIPS 2019|Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting [~Paper](https://arxiv.org/pdf/1902.07379.pdf)|针对DNN容易overfit corrupted labels & class imbalance in training data，提出了一种adaptively reweight的方法，根据training loss and dev loss来优化生成weight的small Meta weight net (MLP)|
|3|Meta Learning|ICML 2017|Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [~Paper](https://arxiv.org/pdf/1703.03400.pdf)|假设一个batch含有m个data point, 传统machine learning直接取这m个 loss的平均值进行一次update，但是MAML是分别对这m个loss更新得到m个updated models, 然后再在一个meta batch(可以理解为mini-dev)上求一个meta loss，得到用于真正update的parameter gradients (second order). 作者在5.2节证明了对second order gradient进行一阶近似效果几乎差不多，但是计算开销大大减少。|
|4|Data Manipulation|NeurIPS 2019|Learning Data Manipulation for Augmentation and Weighting [~Paper](https://arxiv.org/pdf/1910.12795.pdf)|
|5|Meta-Learning|arXiv 2020|Yet Meta Learning Can Adapt Fast, It Can Also Break Easily [~Paper](https://arxiv.org/pdf/2009.01672.pdf)|
|6|Meta-Learning|arXiv 2018|On First-Order Meta-Learning Algorithms.  [~Paper](https://arxiv.org/pdf/1803.02999.pdf)|
