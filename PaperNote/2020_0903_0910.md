# Paper Weekly

> 2020/9/3-9/10

|Index|Area|Source|Title|Idea|
|:---:|:---------:|:--|:---|:----|
|1|Data Subsampling|AAAI 2020|Less Is Better: Unweighted Data Subsampling via Influence Function [~Paper](https://arxiv.org/pdf/1912.01321.pdf)|**What:** 在Big Data上训练complex model是很困难的，本文提出了一种data subsampling的方法旨在利用subset data达到超越full set的效果. **How:** 利用Influence function来获得each training point对entire test loss的影响 (weight)，然后根据此影响对training set进行采样|
|2|Data Subsampling|NeurIPS 2019|Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting [~Paper](https://arxiv.org/pdf/1902.07379.pdf)|针对DNN容易overfit corrupted labels & class imbalance in training data，提出了一种adaptively reweight的方法，根据training loss and dev loss来优化生成weight的small Meta weight net (MLP)|
|3|Meta Learning|ICML 2017|Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [~Paper](https://arxiv.org/pdf/1703.03400.pdf)|假设一个batch含有m个data point, 传统machine learning直接取这m个 loss的平均值进行一次update，但是MAML是分别对这m个loss更新得到m个updated models, 然后再在一个meta batch(可以理解为mini-dev)上求一个meta loss，得到用于真正update的parameter gradients (second order). 作者在5.2节证明了对second order gradient进行一阶近似效果几乎差不多，但是计算开销大大减少。|
|4|Data Manipulation|NeurIPS 2019|Learning Data Manipulation for Augmentation and Weighting [~Paper](https://arxiv.org/pdf/1910.12795.pdf)|前人的data manipulation Algorithms往往只能完成data augmentation 或 data weighting, this paper可以利用一种算法同时实现两种不同的data manipulation (data augmentation: 在不改变label的情况下，perturbs examples以此获得更多的training samples). 个人认为比较有意思的一点:他们认为Maximum Likelihood Learning可以被看做是reinforcement learning的一种特例，因此本文将MLL as a reward learning. (这块还没有完全看懂，需要再好好研究下)|
|5|Meta-Learning|arXiv 2020|Yet Meta Learning Can Adapt Fast, It Can Also Break Easily [~Paper](https://arxiv.org/pdf/2009.01672.pdf)|
|6|Meta-Learning|arXiv 2018|On First-Order Meta-Learning Algorithms.  [~Paper](https://arxiv.org/pdf/1803.02999.pdf)|
|7|Data Reweight|ICML 2018|Learning to Reweight Examples for Robust Deep Learning. [~Paper](https://arxiv.org/pdf/1803.09050.pdf)|
|8|Noisy Supervision|ICML 2020|Searching to Exploit Memorization Effect in Learning with Noisy Labels. [~Paper](https://arxiv.org/pdf/1911.02377.pdf)|
|9|Sequence Prediction|arXiv 2019|Connecting the Dots Between MLE and RL for Sequence Prediction [~Paper](https://arxiv.org/pdf/1811.09740.pdf)|
