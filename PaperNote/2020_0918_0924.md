# Paper Weekly

> 2020/9/18-9/24

|Index|Area|Title & Source|Idea|
|:---:|:--|:-------------|:---|
|1|IR|Learning Deep Structured Semantic Models for Web Search using Clickthrough Data. *CIKM 2013.* [Paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)|算是milestone paper，第一次用supervises neural model在click-relevance data上optimize IR task，并且使用word hashing来缓解semantic model面临的scalability challenge regarding large-scale matrix multiplication. *SIGIR 2018.* [~Paper](https://arxiv.org/pdf/1805.05737.pdf)|***- Motivation.*** 根据TREC ad hoc retreival task对relevance的定义，user's query可以之于whole doc相关，也可以仅对a piece of doc. 因此我们需要可以model diverse relevance pattern -> access relevance granularity. ***- Previous Problems.*** 可以根据relevance粒度将前人工作分为3种: Document-wide Approaches:例如BM25/language model & LeToR ( difficult to model fine-granularity relevance signals)； Passage-level Approaches(大多采用simplified aggregation strategies, 无法为ifferent query-document pairs捕获diverse relevance patterns); Hybrid Approaches: 采用heuristic combination strategies效果不好. ***- Propose.*** 提出一种Hierarchical Nueral Model，利用two stack : (1) local matching (query - each passage of doc) (2) global matching: 测试了3种方法 independent decision即max-pooling, accumulate ~ (用LSTM把各段串起来) (3)Hybrid: 综合前2种 (实验结果显示第3种方法效果最好)|
|3|Neu-IR|DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval *SIGIR 2019.* [~Paper](https://arxiv.org/pdf/1710.05649.pdf)|***-Motivation.*** 现存的Neural Ranker都致力于得到query & doc representations, then generate the ranking score using their similarity; 这样ingore the most important matching characterics like (i) exact matching (ii) query term importance (iii) diverse matching requirement. human judgement process过程有3步：(i) 首先依据query term去doc中detect relevant location (ii) 然后determine local relevance (iii) 最后aggregate local relevance to relevance label. ***-Novelty.*** 本文提出了DeepRank model，对应上述3个steps分别设计了3个module:detect exact maching context + measure network to determine local relevant score + aggregate. ***-Prons.*** 这篇paper对前人工作的总结很不错，比如why the maunal feature engineering hinders IR development(citation): time-consuming, incomplete, over-specific.
|4|Meta-Learning|Optimizing Data Usage via Differentiable Rewards. *ICML 2019.* [~Paper](https://arxiv.org/pdf/1911.10088.pdf)|
|5|Meta-Leanring|Understanding short-horizon bias in stochastic meta-optimization. *ICLR 2018* [~Paper](https://arxiv.org/pdf/1803.02021.pdf)|
|6|follow Meta-Weight-Net|Learning Adaptive Loss for Robust Learning with Noisy Labels. *ArXiv 2020.* [~Paper](https://arxiv.org/pdf/2002.06482.pdf)|
|7|follow Meta-Weight-Net|Learning to Teach with Deep Interactions. *ArXiv 2020.* [~Paper](https://arxiv.org/pdf/2007.04649.pdf)|
|8|Few-shot|Prototypical Networks for Few-shot Learning. *NIPS 2017.* [~Paper](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf)|
|9|follow Meta-Weight-Net|Learning to Match Distributions for Domain Adaptation. *ArXiv 2020.* [~Paper](https://arxiv.org/pdf/2007.10791.pdf)|
|10|transfer learning|Transfer Learning via Learning to Transfer. *ICML 2018.* [~Paper](http://proceedings.mlr.press/v80/wei18a/wei18a.pdf)|
|11|few-shot|Meta-Transfer Learning for Few-Shot Learning. *CVPR 2019.* [~Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf)|
|12|few-shot review|Small Sample Learning in Big Data Era. *ArXiv 2018.* [~Paper](https://arxiv.org/pdf/1808.04572.pdf)|
|13|meta-learning|Revisiting Meta-Learning as Supervised Learning. *ICML 2019.* [~Paper](https://arxiv.org/pdf/2002.00573.pdf)|
|14|transfer-learning|Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks. *ACL 2020.* [~Paper](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)|针对low- and high-resource settings, thorough analysis of domain- and taskadaptive pretraining，证明其有效性. 并运用了a simple data selection strategy (nearest neighbors selection)来帮助更好的pretraining.|
|15|LeToR|The PageRank Citation Ranking: Bringing Order to the Web. *1998.* [~Paper](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf)|
