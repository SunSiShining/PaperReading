# Paper Weekly

> 2020/8/31-9/6

- **Understanding black-box predictions via influence functions / 2017** <a href="#1-Understanding">~Note</a> [~Paper](https://arxiv.org/pdf/1703.04730.pdf) **Ù©(à¹‘>â—¡<à¹‘)Û¶ BEST RECOMMEND Ù©(à¹‘>â—¡<à¹‘)Û¶**
- **ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search / 2020** <a href="#2-ORCAS">~Note</a> [~Paper](https://arxiv.org/pdf/1703.04730.pdf)
- **Dense Passage Retrieval for Open-Domain Question Answering / 2020** <a href="#3-Dense">~Note</a> [~Paper](https://arxiv.org/pdf/2004.04906.pdf)
- **Complementing Lexical Retrieval with Semantic Residual Embedding / 2020** <a href="#4-Complementing">~Note</a> [~Paper](https://arxiv.org/pdf/2004.13969.pdf)
- **Neural Models for Information Retrieval / 2017** [~Paper](https://arxiv.org/pdf/1705.01509.pdf)
- **A Deep Look into Neural Ranking Models for Information Retrieval / 2020** [~Paper](https://arxiv.org/pdf/1903.06902.pdf)
- **Enhancing Simple Models by Exploiting What They Already Know** [~Paper](https://proceedings.icml.cc/static/paper_files/icml/2020/126-Paper.pdf)


## [1. Understanding](#contents)

### **Understanding black-box predictions via influence functions** [~Paper](https://arxiv.org/pdf/1703.04730.pdf) [~Video](https://www.youtube.com/watch?v=0w9fLX_T6tY)

### >> Abstract

å¦‚ä½•è§£é‡Š a black-box modelçš„predictionï¼Ÿæœ¬æ–‡é‡‡ç”¨influence functionsï¼Œè¿½è¸ªmodel prediction to training data, æœ€ç»ˆç¡®å®šåˆ°åº•å“ªä¸ªdataå¯¹model preidictionè‡³å…³é‡è¦ã€‚

ä¸ºäº†å¯¹machine learningçš„settingè¿ç”¨influence functions, æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ä»…éœ€access to gradients and Hessian-vector products.

å³ä½¿åœ¨non-convex and non-differentiable modelä¸Šï¼Œinfluence functionsä»æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

æœ¬æ–‡åœ¨Linear model and Convolution neural networkä¸ŠéªŒè¯äº†influence functionså¯ä»¥ç”¨äºï¼š
- Understanding model behavior
- debugging model
- detecting model behavior
- create visually-indistinguishable training-set attacks

ğŸ¤” æ‰€ä»¥ä»€ä¹ˆæ˜¯influence functions? å®ƒåˆæ˜¯å¦‚ä½•åˆ©ç”¨gradientsæ¥trac model predictionsçš„ï¼Ÿå¥½å¥‡ä½œè€…æ€ä¹ˆåœ¨modelä¸ŠéªŒè¯æåˆ°çš„å‡ ç§ç”¨é€”ï¼Ÿ

### >> Introduction

åœ¨è§£é‡Šblack-box modelä¸Šå·²æœ‰çš„å·¥ä½œå¤§å¤šç ”ç©¶: ä¸€ä¸ªå›ºå®šçš„modelï¼Œå¦‚ä½•åšå‡ºç‰¹å®šçš„é¢„æµ‹, æ¯”å¦‚å¯¹test pointè¿›è¡Œæ‰°åŠ¨ï¼Œçœ‹çœ‹predictionå¦‚ä½•å˜åŒ–ã€‚æœ¬æ–‡é€šè¿‡modelçš„learning algorithmè¿½è¸ªmodel predictionæœ€ç»ˆback to its training data.

ä¸ºäº†åˆ¤æ–­a training pointå¯¹a predictionçš„å½±å“ï¼Œæˆ‘ä»¬å¯ä»¥é—®è¿™ä¹ˆä¸€ä¸ªé—®é¢˜: å¦‚æœæ²¡æœ‰è¿™ä¸ªtraining pointä¼šæ€ä¹ˆæ ·? ä½†æ˜¯å¦‚æœä¸ºäº†æµ‹è¯•a training dataå°±é‡æ–°è®­ä¸€émodelè¿™ä»£ä»·ä¹Ÿå¤ªå¤§äº†ï¼Œå› æ­¤ä½œè€…ä½¿ç”¨äº†influence functions (a classic technique from robust statistics, 1980)ï¼Œå®ƒå‘Šè¯‰äº†æˆ‘ä»¬å¦‚æœå¯¹a training pointè¿›è¡Œæ— ç©·å°çš„upweightï¼Œmodel parameterå¦‚ä½•å˜åŒ–ã€‚

ä½†influence functionçš„åº”ç”¨éšœç¢æ˜¯: expensive second derivative calculations and assume model differentiability and convexity.

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨second-order optimzation æŠ€æœ¯å¯¹influence functionsè¿›è¡Œè¿›è¡Œï¼Œå³ä½¿åœ¨ä¸å¯å¾®æˆ–éå‡¸é—®é¢˜ä¸Šéƒ½å…·å¤‡accurate.

### >> Approach

é¦–å…ˆä»‹ç»äº†previous influence functionæ˜¯æ€ä¹ˆåšçš„ï¼šupweight training point, evaluate on test setï¼Œç„¶ååœ¨2.1èŠ‚è¿›è¡Œå»¶æ‹“æˆå¯¹training pointè¿›è¡ŒPerturbing



## [2. ORCAS](#contents)

### **ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search** [~Paper](https://arxiv.org/pdf/1703.04730.pdf)

æœ¬æ–‡å‘å¸ƒäº†ä¸€ä¸ªpublicçš„click logs datasetï¼Œä¸TREC Deep Learning Track ç›¸å…³ã€‚æ–‡ä¸­æœ‰ä¸ªä¾‹å­æŒºæœ‰æ„æ€ï¼Œquery Pandaså¯ä»¥linkåˆ°å¾ˆå¤šç›¸å…³topic: ç†ŠçŒ«ã€æ­Œæ›²ç­‰ç­‰ï¼Œå¦‚ä½•ç¡®å®šmost query-realted docsï¼Œä½¿ç”¨user clickå°±å‘ç°æ¾³æ´²çš„ç½‘ç»œæ›´å¯èƒ½linkåˆ°"ç†ŠçŒ«æ±‚åŠ©çƒ­çº¿"ï¼Œä½†æ˜¯ç¾å›½ç½‘é¡µå°±ä¸æ˜¯ã€‚ä½†æ˜¯ä»…å­˜çš„å¼€æºuser click datasetéƒ½æ˜¯IDåŒ¿ååŒ–çš„ï¼Œå®ƒå°±æ²¡æ³•æä¾›æä¾›è¿™æ–¹é¢çš„ä¿¡æ¯ï¼Œä½†æ˜¯æ­¤paper releaseçš„Open Resource for Click Analysis in Search (ORCAS)ï¼Œä¸ä»…å¯ä»¥æä¾› user-level or session-level informationï¼Œè¿˜å¯ä»¥release without privacy violation.

ä½†æ˜¯æœ¬æ–‡åˆ©ç”¨ORCAS +  TREC DL dataå…±åŒtrain rankerç«Ÿç„¶no statistically significant gains. Possible Future work: Correctly weighting and adjusting the data to achieve a significant gain on a test set of 43 queries is left as future work.



## [3. Dense](#contents)

### **Dense Passage Retrieval for Open-Domain Question Answering** [~Paper](https://arxiv.org/pdf/2004.04906.pdf)

Open-domain QA ä¾èµ–ä¼ ç»Ÿçš„sparse retrieverï¼ˆBM25ï¼‰è·å¾—candidate passage subsetã€‚è¿™ç¯‡å·¥ä½œå±•ç¤ºäº†å¯ä»¥åˆ©ç”¨a simple dual-encoder frameworkå³å¯å®ç°dense passage retrevalã€‚åœ¨a wide range of open-domain QA datasetä¸ŠéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•outperforms BM25 by 9%~19% (top@20)


## [4. Complementing](#contents)

### **Complementing Lexical Retrieval with Semantic Residual Embedding** [~Paper](https://arxiv.org/pdf/2004.13969.pdf)

sparse retrievalå›ºæœ‰çš„vocabulary mismatch & topic-level matching; neural embeddingè™½ç„¶å¯ä»¥æä¾›semantic spaceçš„å…³è”ä½†æ˜¯å¤±å»äº†token-levelçš„matching that is critical to IR. æœ¬å·¥ä½œæ—¨åœ¨èåˆSparse + Nueral, å¹¶ä¸”åˆ©ç”¨residual-based embeddingæ¥æ”¹å–„é‚£äº›deep languageæ— æ³•captureçš„lexical retrival. ç»“æœæ˜¾ç¤ºæ­¤æ–¹æ³•å¯ä»¥è¶…è¶Š bag-of-words retrieval models, BERT-enhaced lexical retrieval models & BERT-based embedding retrieval.

æå‡ºä¸¤ç§residual-based learning methods: Error-based Negative Sampling & Residual-based Margin

- Error-based Negative Sampling: å¯¹è´Ÿä¾‹è¿›è¡Œé‡‡æ ·ï¼Œé€‰æ‹©é‚£äº›è¢«BM25æŠ½å›(lexical matching)ï¼Œä½†æ˜¯ä¸ç›¸å…³çš„doc (semantically irrelevant)
- Residual-based Margin: åˆ©ç”¨lexical retrievalå†³å®šdense retrieverå¯¹ä¸åŒtraining tripleçš„ranking margin. æ¯”å¦‚å¦‚æœlexical retrieverå¯¹æ­¤triple rankçš„æ¯”è¾ƒå¥½ï¼Œé‚£ä¹ˆç›¸åº”çš„marginå°±æ¯”è¾ƒå°ï¼Œåä¹‹äº¦ç„¶ã€‚
