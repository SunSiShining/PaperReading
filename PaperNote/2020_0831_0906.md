# Paper Weekly

> 2020/8/31-9/6

- **Understanding black-box predictions via influence functions / 2017** <a href="#1-Understanding">~Note</a> [~Paper](https://arxiv.org/pdf/1703.04730.pdf) **٩(๑>◡<๑)۶ BEST RECOMMEND ٩(๑>◡<๑)۶**
- **ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search / 2020** <a href="#2-ORCAS">~Note</a> [~Paper](https://arxiv.org/pdf/1703.04730.pdf)
- **Dense Passage Retrieval for Open-Domain Question Answering / 2020** <a href="#3-Dense">~Note</a> [~Paper](https://arxiv.org/pdf/2004.04906.pdf)
- **Complementing Lexical Retrieval with Semantic Residual Embedding / 2020** <a href="#4-Complementing">~Note</a> [~Paper](https://arxiv.org/pdf/2004.13969.pdf)
- **Neural Models for Information Retrieval / 2017** [~Paper](https://arxiv.org/pdf/1705.01509.pdf)
- **A Deep Look into Neural Ranking Models for Information Retrieval / 2020** [~Paper](https://arxiv.org/pdf/1903.06902.pdf)
- **Enhancing Simple Models by Exploiting What They Already Know** [~Paper](https://proceedings.icml.cc/static/paper_files/icml/2020/126-Paper.pdf)


## [1. Understanding](#contents)

### **Understanding black-box predictions via influence functions** [~Paper](https://arxiv.org/pdf/1703.04730.pdf) [~Video](https://www.youtube.com/watch?v=0w9fLX_T6tY)

### >> Abstract

如何解释 a black-box model的prediction？本文采用influence functions，追踪model prediction to training data, 最终确定到底哪个data对model preidiction至关重要。

为了对machine learning的setting运用influence functions, 本文提出一种方法仅需access to gradients and Hessian-vector products.

即使在non-convex and non-differentiable model上，influence functions仍提供了有价值的信息。

本文在Linear model and Convolution neural network上验证了influence functions可以用于：
- Understanding model behavior
- debugging model
- detecting model behavior
- create visually-indistinguishable training-set attacks

🤔 所以什么是influence functions? 它又是如何利用gradients来trac model predictions的？好奇作者怎么在model上验证提到的几种用途？

### >> Introduction

在解释black-box model上已有的工作大多研究: 一个固定的model，如何做出特定的预测, 比如对test point进行扰动，看看prediction如何变化。本文通过model的learning algorithm追踪model prediction最终back to its training data.

为了判断a training point对a prediction的影响，我们可以问这么一个问题: 如果没有这个training point会怎么样? 但是如果为了测试a training data就重新训一遍model这代价也太大了，因此作者使用了influence functions (a classic technique from robust statistics, 1980)，它告诉了我们如果对a training point进行无穷小的upweight，model parameter如何变化。

但influence function的应用障碍是: expensive second derivative calculations and assume model differentiability and convexity.

我们可以利用second-order optimzation 技术对influence functions进行进行，即使在不可微或非凸问题上都具备accurate.

### >> Approach

首先介绍了previous influence function是怎么做的：upweight training point, evaluate on test set，然后在2.1节进行延拓成对training point进行Perturbing



## [2. ORCAS](#contents)

### **ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search** [~Paper](https://arxiv.org/pdf/1703.04730.pdf)

本文发布了一个public的click logs dataset，与TREC Deep Learning Track 相关。文中有个例子挺有意思，query Pandas可以link到很多相关topic: 熊猫、歌曲等等，如何确定most query-realted docs，使用user click就发现澳洲的网络更可能link到"熊猫求助热线"，但是美国网页就不是。但是仅存的开源user click dataset都是ID匿名化的，它就没法提供提供这方面的信息，但是此paper release的Open Resource for Click Analysis in Search (ORCAS)，不仅可以提供 user-level or session-level information，还可以release without privacy violation.

但是本文利用ORCAS +  TREC DL data共同train ranker竟然no statistically significant gains. Possible Future work: Correctly weighting and adjusting the data to achieve a significant gain on a test set of 43 queries is left as future work.



## [3. Dense](#contents)

### **Dense Passage Retrieval for Open-Domain Question Answering** [~Paper](https://arxiv.org/pdf/2004.04906.pdf)

Open-domain QA 依赖传统的sparse retriever（BM25）获得candidate passage subset。这篇工作展示了可以利用a simple dual-encoder framework即可实现dense passage retreval。在a wide range of open-domain QA dataset上验证我们的方法outperforms BM25 by 9%~19% (top@20)


## [4. Complementing](#contents)

### **Complementing Lexical Retrieval with Semantic Residual Embedding** [~Paper](https://arxiv.org/pdf/2004.13969.pdf)

sparse retrieval固有的vocabulary mismatch & topic-level matching; neural embedding虽然可以提供semantic space的关联但是失去了token-level的matching that is critical to IR. 本工作旨在融合Sparse + Nueral, 并且利用residual-based embedding来改善那些deep language无法capture的lexical retrival. 结果显示此方法可以超越 bag-of-words retrieval models, BERT-enhaced lexical retrieval models & BERT-based embedding retrieval.

提出两种residual-based learning methods: Error-based Negative Sampling & Residual-based Margin

- Error-based Negative Sampling: 对负例进行采样，选择那些被BM25抽回(lexical matching)，但是不相关的doc (semantically irrelevant)
- Residual-based Margin: 利用lexical retrieval决定dense retriever对不同training triple的ranking margin. 比如如果lexical retriever对此triple rank的比较好，那么相应的margin就比较小，反之亦然。
