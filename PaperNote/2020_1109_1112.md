# Paper Weekly

> 2020/11/2-11/05

|Index|Area|Title & Source|Idea|
|:---:|:--|:-------------|:---|
|1|End-to-End Retrieval|Dc-bert: Decoupling question and document for efficient contextual encoding. *Nie, Ping and Zhang, Yuyu and Geng, Xiubo and Ramamurthy, Arun and Song, Le and Jiang, Daxin.* SIGIR 2020. [[paper]](https://arxiv.org/pdf/2002.12591.pdf)|针对BERT-Reranker需要concate Q+D Interaction速度慢的问题 (难以并行interact 多个Queries与大量docs)；提出Dual-BERT: (1) online BERT encode Q, (2) offline BERT encode docs (3) a transformer layer interact encode (Q + D) (4) binary classification|
|2|Rerank Effectiveness|**Efficient Document Re-Ranking for Transformers by Precomputing Term Representations.** *MacAvaney, Sean and Nardini, Franco Maria and Perego, Raffaele and Tonellotto, Nicola and Goharian, Nazli and Frieder, Ophir.* arXiv 2020. [[paper]](https://arxiv.org/pdf/2004.14255.pdf)|**提出:** (1)将doc term representations precomputed and stored in the index stage, 然后在query time在计算query term and doc term inertaction top-layers of transformer实现speedup; 由于需要store doc term representations，提出在最后几层transformer layer前先用an extra layer (feed-forward & normalized) 对term vectors进行压缩，然后，optimize该compress network的目标为reduce the mean squared error of the attention score between added transformer layers and previous transformer layers.|
