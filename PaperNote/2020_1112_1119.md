# Paper Weekly

> 2020/11/12-11/19

|Index|Area|Title & Source|Idea|
|:---:|:--|:-------------|:---|
|1|Relevance Feedback|**A Reinforcement Learning Framework for Relevance Feedback.** *Montazeralghaem, Ali and Zamani, Hamed and Allan, James.* SIGIR 2020. [[paper]](http://maroo.cs.umass.edu/pub/web/getpdf.php?id=1343)|**Motivation:** search query are often too short or well speicifc but no match the vocabulary, relevance feedback is an effective technique to solve it. **Related work:** relevance feedback有很多形式，比如document relevance judgements (explict), Clickthrough data or pseudo-relevance feedback (implict)等. **Propose:** 提出使用RL方法利用relevance feedback to add/reweight query terms, 并直接优化IR metrics.|
|2|Contrastive Learning|**Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning.** *Gunel, Beliz and Du, Jingfei and Conneau, Alexis and Stoyanov, Ves.* arXiv 2020. [[paper]](https://arxiv.org/pdf/2011.01403.pdf)|**Motivation:** 针对classifiction task on pretrained model时使用Crossentropy loss带来的generlazation & robustness差的问题， 提出a contrastive objective to push same-class example close, and different-class examples apart. **Related work:** 除了Contrastive Learning，还提到了Stability and robustness of fine-tuning language models. **Propose:** Contrastvie loss + Crossentropy loss在RoBERTa上取得了一致性提升，并且对high/low-data regimes, noisy data augmentation都表现出robustness.|
|3|Rank fusion|**Reciprocal rank fusion outperforms condorcet and individual rank learning methods.**  *Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan.* SIGIR 2009. [[paper]](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)|了解了一下Reciprocal rank fusion的intuition做法: highly-ranked docs are more important, but the importance of lower-ranked docs does not vanish.  其中有一个参数k对结果有较大影响。|
||
||BERT Analysis|**What does BERT look at? An Analysis of BERT’s Attention.** *Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.* ACL 2019. [[paper]](https://www.aclweb.org/anthology/W19-48.pdf#page=290)|
||Pairwise softmax loss|**Neural Ranking Models with Weak Supervision.** *Dehghani, Mostafa and Zamani, Hamed and Severyn, Aliaksei and Kamps, Jaap and Croft, W Bruce.* SIGIR 2017. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3077136.3080832)|
||
||Variational Autoencoder|OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space. [[paper]](https://arxiv.org/pdf/2004.04092.pdf)|
||Doc expansion|Document expansion by query prediction. *Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho.* arXiv 2019. [[paper]](https://arxiv.org/pdf/1904.08375.pdf)
||Doc expansion|From doc2query to docTTTTTquery. *Nogueira, Rodrigo and Lin, Jimmy and Epistemic, AI.* arXiv 2019. [[paper]](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery.pdf)|
